{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Creation_date</th>\n",
       "      <th>Name</th>\n",
       "      <th>User</th>\n",
       "      <th>location</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minha fam√≠lia √© louca demais kkk\\nMeu tio perg...</td>\n",
       "      <td>Pablo Lemes</td>\n",
       "      <td>sou_pablo2</td>\n",
       "      <td>29/11/2020 18:01</td>\n",
       "      <td>Bras√≠lia, Brasil</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>271</td>\n",
       "      <td>244</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Se √© azul e fala, s√≥ pode ser uma arara https:...</td>\n",
       "      <td>MMD&amp;Doider üíÉ</td>\n",
       "      <td>BeckieApenas</td>\n",
       "      <td>29/11/2020 17:18</td>\n",
       "      <td>Manaus, Brazil</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E ontem o cara me aparece com uma arara azul n...</td>\n",
       "      <td>Vanderson ü¶Ö</td>\n",
       "      <td>Vaandoo_</td>\n",
       "      <td>29/11/2020 14:55</td>\n",
       "      <td>S√£o Bernardo do Campo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>376</td>\n",
       "      <td>577</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@hallwayghost KAKAKKAKA a arara azul era de um...</td>\n",
       "      <td>duda ‚Ä¢ üìñ: hdo‚Åµ</td>\n",
       "      <td>twentyhalpert</td>\n",
       "      <td>29/11/2020 03:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6652</td>\n",
       "      <td>6693</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@twentyhalpert eu moro interior to acostumada ...</td>\n",
       "      <td>p√¢mela</td>\n",
       "      <td>hallwayghost</td>\n",
       "      <td>29/11/2020 03:40</td>\n",
       "      <td>üè° = frens</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>586</td>\n",
       "      <td>641</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet   Creation_date  \\\n",
       "0  Minha fam√≠lia √© louca demais kkk\\nMeu tio perg...     Pablo Lemes   \n",
       "1  Se √© azul e fala, s√≥ pode ser uma arara https:...    MMD&Doider üíÉ   \n",
       "2  E ontem o cara me aparece com uma arara azul n...     Vanderson ü¶Ö   \n",
       "3  @hallwayghost KAKAKKAKA a arara azul era de um...  duda ‚Ä¢ üìñ: hdo‚Åµ   \n",
       "4  @twentyhalpert eu moro interior to acostumada ...          p√¢mela   \n",
       "\n",
       "            Name              User               location  retweets  \\\n",
       "0     sou_pablo2  29/11/2020 18:01       Bras√≠lia, Brasil         0   \n",
       "1   BeckieApenas  29/11/2020 17:18         Manaus, Brazil         0   \n",
       "2       Vaandoo_  29/11/2020 14:55  S√£o Bernardo do Campo         0   \n",
       "3  twentyhalpert  29/11/2020 03:44                    NaN         0   \n",
       "4   hallwayghost  29/11/2020 03:40              üè° = frens         0   \n",
       "\n",
       "   favorites  friends_count  followers_count  Classification  \n",
       "0          1            271              244              -1  \n",
       "1          0            192              105               0  \n",
       "2          1            376              577              -1  \n",
       "3          0           6652             6693              -1  \n",
       "4          0            586              641               0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Datasets\\AraraAzul_Class_20201129.csv\", delimiter=\";\", encoding=\"utf-8\") # Utiliza√ß√£o do dataset classificado manualmente com os Tweets\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o dos Embeddings\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "# Importando os Embeddings e transformando num array\n",
    "\n",
    "## tf-idf\n",
    "\n",
    "tf_idf_emb = corpora.MmCorpus('Embeddings\\TF_IDF\\matriz_tfidf.mm')\n",
    "tf_idf_emb = gensim.matutils.corpus2csc(tf_idf_emb)\n",
    "tf_idf_emb = tf_idf_emb.T.toarray()\n",
    "\n",
    "## LSA\n",
    "\n",
    "lsa_emb_50 = corpora.MmCorpus('Embeddings\\LSA_LSI\\matriz_lsi_lsa_50.mm')\n",
    "lsa_emb_100 = corpora.MmCorpus('Embeddings\\LSA_LSI\\matriz_lsi_lsa_100.mm')\n",
    "lsa_emb_150 = corpora.MmCorpus('Embeddings\\LSA_LSI\\matriz_lsi_lsa_150.mm')\n",
    "lsa_emb_200 = corpora.MmCorpus('Embeddings\\LSA_LSI\\matriz_lsi_lsa_200.mm')\n",
    "lsa_emb_250 = corpora.MmCorpus('Embeddings\\LSA_LSI\\matriz_lsi_lsa_250.mm')\n",
    "lsa_emb_300 = corpora.MmCorpus('Embeddings\\LSA_LSI\\matriz_lsi_lsa_300.mm')\n",
    "\n",
    "lsa_emb_50 = gensim.matutils.corpus2csc(lsa_emb_50)\n",
    "lsa_emb_100 = gensim.matutils.corpus2csc(lsa_emb_100)\n",
    "lsa_emb_150 = gensim.matutils.corpus2csc(lsa_emb_150)\n",
    "lsa_emb_200 = gensim.matutils.corpus2csc(lsa_emb_200)\n",
    "lsa_emb_250 = gensim.matutils.corpus2csc(lsa_emb_250)\n",
    "lsa_emb_300 = gensim.matutils.corpus2csc(lsa_emb_300)\n",
    "\n",
    "lsa_emb_50 = lsa_emb_50.T.toarray()\n",
    "lsa_emb_100 = lsa_emb_100.T.toarray()\n",
    "lsa_emb_150 = lsa_emb_150.T.toarray()\n",
    "lsa_emb_200 = lsa_emb_200.T.toarray()\n",
    "lsa_emb_250 = lsa_emb_250.T.toarray()\n",
    "lsa_emb_300 = lsa_emb_300.T.toarray()\n",
    "\n",
    "\n",
    "## TopicLSA\n",
    "\n",
    "Topic_LSA_emb_50 = corpora.MmCorpus('Embeddings\\TOPIC_LSA\\Topic_lsa_50.mm')\n",
    "Topic_LSA_emb_100 = corpora.MmCorpus('Embeddings\\TOPIC_LSA\\Topic_lsa_100.mm')\n",
    "Topic_LSA_emb_150 = corpora.MmCorpus('Embeddings\\TOPIC_LSA\\Topic_lsa_150.mm')\n",
    "Topic_LSA_emb_200 = corpora.MmCorpus('Embeddings\\TOPIC_LSA\\Topic_lsa_200.mm')\n",
    "Topic_LSA_emb_250 = corpora.MmCorpus('Embeddings\\TOPIC_LSA\\Topic_lsa_250.mm')\n",
    "Topic_LSA_emb_300 = corpora.MmCorpus('Embeddings\\TOPIC_LSA\\Topic_lsa_300.mm')\n",
    "\n",
    "Topic_LSA_emb_50 = gensim.matutils.corpus2csc(Topic_LSA_emb_50)\n",
    "Topic_LSA_emb_100 = gensim.matutils.corpus2csc(Topic_LSA_emb_100)\n",
    "Topic_LSA_emb_150 = gensim.matutils.corpus2csc(Topic_LSA_emb_150)\n",
    "Topic_LSA_emb_200 = gensim.matutils.corpus2csc(Topic_LSA_emb_200)\n",
    "Topic_LSA_emb_250 = gensim.matutils.corpus2csc(Topic_LSA_emb_250)\n",
    "Topic_LSA_emb_300 = gensim.matutils.corpus2csc(Topic_LSA_emb_300)\n",
    "\n",
    "Topic_LSA_emb_50 = Topic_LSA_emb_50.T.toarray()\n",
    "Topic_LSA_emb_100 = Topic_LSA_emb_100.T.toarray()\n",
    "Topic_LSA_emb_150 = Topic_LSA_emb_150.T.toarray()\n",
    "Topic_LSA_emb_200 = Topic_LSA_emb_200.T.toarray()\n",
    "Topic_LSA_emb_250 = Topic_LSA_emb_250.T.toarray()\n",
    "Topic_LSA_emb_300 = Topic_LSA_emb_300.T.toarray()\n",
    "\n",
    "## LDA\n",
    "\n",
    "lda_emb_50 = corpora.MmCorpus('Embeddings\\LDA\\Lda_model_50.mm')\n",
    "lda_emb_100 = corpora.MmCorpus('Embeddings\\LDA\\Lda_model_100.mm')\n",
    "lda_emb_150 = corpora.MmCorpus('Embeddings\\LDA\\Lda_model_150.mm')\n",
    "lda_emb_200 = corpora.MmCorpus('Embeddings\\LDA\\Lda_model_200.mm')\n",
    "lda_emb_250 = corpora.MmCorpus('Embeddings\\LDA\\Lda_model_250.mm')\n",
    "lda_emb_300 = corpora.MmCorpus('Embeddings\\LDA\\Lda_model_300.mm')\n",
    "\n",
    "lda_emb_50 = gensim.matutils.corpus2csc(lda_emb_50)\n",
    "lda_emb_100 = gensim.matutils.corpus2csc(lda_emb_100)\n",
    "lda_emb_150 = gensim.matutils.corpus2csc(lda_emb_150)\n",
    "lda_emb_200 = gensim.matutils.corpus2csc(lda_emb_200)\n",
    "lda_emb_250 = gensim.matutils.corpus2csc(lda_emb_250)\n",
    "lda_emb_300 = gensim.matutils.corpus2csc(lda_emb_300)\n",
    "\n",
    "lda_emb_50 = lda_emb_50.T.toarray()\n",
    "lda_emb_100 = lda_emb_100.T.toarray()\n",
    "lda_emb_150 = lda_emb_150.T.toarray()\n",
    "lda_emb_200 = lda_emb_200.T.toarray()\n",
    "lda_emb_250 = lda_emb_250.T.toarray()\n",
    "lda_emb_300 = lda_emb_300.T.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: TFIDF \n",
      "Modelo: Naive Bayes \n",
      "Acur√°cia:  0.7021276595744681\n"
     ]
    }
   ],
   "source": [
    "# Divis√£o da base de treino e base de teste (tfidf)\n",
    "\n",
    "X = tf_idf_emb\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Treinando o Modelo\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predi√ß√£o \n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Extraindo a acur√°cia do modelo\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Acuracia = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print (\"Embedding: TFIDF \\n\"+\"Modelo: Naive Bayes \\n\"+\"Acur√°cia: \", Acuracia)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('uspenv': conda)",
   "language": "python",
   "name": "python37764bituspenvconda3aaf4a122e974252b54ca0bba236eb6c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
